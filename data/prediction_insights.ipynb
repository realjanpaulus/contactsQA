{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f25aea",
   "metadata": {},
   "source": [
    "# Prediction insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "from statsmodels.stats.contingency_tables import mcnemar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592b8ea",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "[1. Analyze errors of one experiment](#1)<br>\n",
    "&nbsp; [1.1. Differences between True and Pred](#1-1)<br>\n",
    "&nbsp; [1.2. Analyze mistakes](#1-2)<br>\n",
    "&nbsp; [1.3. Best answer with probability](#1-3)<br>\n",
    "&nbsp; [1.4. Search for  faulty testcases](#1-4)<br>\n",
    "[2. Compare errors of two experiments](#2)<br>\n",
    "&nbsp; [2.1. Differences between True and Pred](#2-1)<br>\n",
    "&nbsp; [2.2. Looking into ids of same and different errors](#2-2)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; [2.2.1. Deeper analysis: Randomly looking into errors](#2-2-1)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; [2.2.2. Deeper analysis: N-Best Prediction analysis](#2-2-2)<br>\n",
    "&nbsp; [2.3. McNemar Test](#2-3)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e4ad71",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f26ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_p_value(diffs1, diffs2):\n",
    "    \n",
    "    # build contigency table\n",
    "    diff1_incorrect_diff2_correct = 0\n",
    "    \n",
    "    for _, row in diffs1.iterrows():\n",
    "        if row[\"id\"] not in diffs2[\"id\"].values:\n",
    "            diff1_incorrect_diff2_correct += 1\n",
    "                    \n",
    "    diff1_correct_diff2_incorrect = 0\n",
    "    \n",
    "    for _, row in diffs2.iterrows():\n",
    "        if row[\"id\"] not in diffs1[\"id\"].values:\n",
    "            diff1_correct_diff2_incorrect += 1\n",
    "    \n",
    "    contigency_table = [\n",
    "        [0, diff1_correct_diff2_incorrect], \n",
    "        [diff1_incorrect_diff2_correct, 0]\n",
    "    ]\n",
    "    \n",
    "    return mcnemar(contigency_table, exact=True, correction=True).pvalue\n",
    "\n",
    "def get_diffs_dfs(new_test):\n",
    "    new_test[\"preds\"] = new_test[\"preds\"].astype(str)\n",
    "    new_test[\"true_answer\"] = new_test[\"true_answer\"].astype(str)\n",
    "    \n",
    "    diffs = new_test[new_test.preds != new_test.true_answer]\n",
    "    \n",
    "    new_test2 = new_test.copy()\n",
    "    new_test2[\"preds\"] = new_test2.apply(lambda row: normalize_answer(row.preds), axis=1)\n",
    "    new_test2[\"true_answer\"] = new_test2.apply(lambda row: normalize_answer(row.true_answer), axis=1)\n",
    "    normalized_diffs = new_test2[new_test2.preds != new_test2.true_answer]\n",
    "    \n",
    "    return diffs, normalized_diffs\n",
    "\n",
    "\n",
    "def get_test_and_preds(path, test_path):\n",
    "    with open(path+\"test_predictions.json\", \"r\") as f:\n",
    "        preds = json.load(f)\n",
    "    \n",
    "    test = pd.read_json(path_or_buf=test_path, lines=True)\n",
    "    return test, preds\n",
    "\n",
    "def get_top1_answer_probability(nbest_preds):\n",
    "    new_nbest_preds = {new_id: l[0] for new_id, l in nbest_preds.items()}\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(new_nbest_preds, orient=\"index\")\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    df.columns = [\"id\", \"start_logit\", \"end_logit\", \"text\", \"probability\"]\n",
    "    return df\n",
    "\n",
    "def join_test_preds(test, preds):\n",
    "    \"\"\"Add the test prediction to the test dataframe.\"\"\"\n",
    "    pred_df = pd.DataFrame(preds.items())\n",
    "    pred_df.columns = [\"id\", \"preds\"]\n",
    "    \n",
    "    new_test = test.set_index('id').join(pred_df.set_index('id'))\n",
    "    new_test.reset_index(level=0, inplace=True)\n",
    "    new_test.columns = [\"id\", \"orig_id\", \"title\", \"context\", \"fixed\", \"question\", \"answers\", \"preds\"]\n",
    "    new_test[\"true_answer\"] = new_test.apply(lambda row: dict(row.answers)[\"text\"][0], axis=1)\n",
    "    del new_test[\"title\"]\n",
    "    \n",
    "    def replace_empty_string(row):\n",
    "        if row[\"preds\"] == \"\":\n",
    "            return \"EMPTY\"\n",
    "        else:\n",
    "            return row[\"preds\"]\n",
    "    \n",
    "    new_test[\"preds\"] = new_test.apply(lambda row: replace_empty_string(row), axis=1)\n",
    "    \n",
    "    return new_test\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    return white_space_fix(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f90380a-75fc-4598-ac45-d732ea87b516",
   "metadata": {},
   "source": [
    "## 1. Analyze errors of one experiment <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b4ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set and preds\n",
    "PATH = \"../results_crawl_na_big_rework_thin/crawl-thin-na/xlm-roberta-base/\"\n",
    "TEST_PATH = f\"crawl-thin-na/crawl-thin-na-test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59ea921",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test, preds = get_test_and_preds(PATH, TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18fe3ae",
   "metadata": {},
   "source": [
    "### 1.1. Differences between True and Pred <a id=\"1-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e3672",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = join_test_preds(test, preds)\n",
    "new_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb44111",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs, normalized_diffs = get_diffs_dfs(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b066d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(diffs)} differences between the true and pred answers (no normalizing)!\")\n",
    "print(f\"There are {len(normalized_diffs)} differences between the true and pred answers (WITH normalizing)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8752c8c-30d3-435e-a2ba-123c110b3615",
   "metadata": {},
   "source": [
    "### 1.2. Analyze mistakes <a id=\"1-2\"></a>\n",
    "\n",
    "Two DataFrames:\n",
    "1. one with all mistakes (`diffs`)\n",
    "2. one without substring answers where neither TRUE is a substring of PRED nor PRED is a substring of TRUE (`diffs_no_sub`). So the following is NOT included:\n",
    "    - PRED: `industrial avenue 22-24` \n",
    "    - TRUE: `industrial avenue 22-24 ilupeju`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddcbe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs_no_sub = new_test[(~new_test.preds.isin(new_test.true_answer))|(~new_test.true_answer.isin(new_test.preds))]\n",
    "\n",
    "print(\"Count of wrong predictions:\", diffs.shape[0])\n",
    "print(\"Count of wrong predictions (no substring match):\", diffs_no_sub.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a961db1-e833-4687-bbeb-1ea956456c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell for a detailed analysis\n",
    "\n",
    "n = input()\n",
    "if n == \"\" or n == \"r\":\n",
    "    n = random.randint(0, len(diffs_no_sub))\n",
    "    print(\"n:\", n)\n",
    "current_error = diffs_no_sub.iloc[int(n)-1:int(n)]\n",
    "context = current_error.iloc[0].context\n",
    "print(f\"Length of the context: {len(context)}\")\n",
    "print(current_error.iloc[0][\"id\"])\n",
    "current_error.loc[:, current_error.columns != 'context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd36656",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_char = -1\n",
    "context[:max_char]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc92480",
   "metadata": {},
   "source": [
    "### 1.3. Best answer with probability <a id=\"1-3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH+\"test_nbest_predictions.json\", \"r\") as f:\n",
    "    nbest_preds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb42967",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = \"69243-0\"\n",
    "answers = [v for k,v in nbest_preds.items() if k.startswith(target_id)]\n",
    "nbest_preds_df = pd.DataFrame(answers[0]).sort_values(by=\"probability\", ascending=False)\n",
    "nbest_preds_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687cd913",
   "metadata": {},
   "source": [
    "Use the following DataFrame to investigate the models probabilites for the top 1 answer (= returned pred answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acd4c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_probs = get_top1_answer_probability(nbest_preds)\n",
    "\n",
    "no_empty_answers = True\n",
    "\n",
    "if no_empty_answers:\n",
    "    top1_probs = top1_probs[top1_probs.text != \"\"]\n",
    "\n",
    "top1_probs.sort_values(by=\"probability\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718e0997",
   "metadata": {},
   "source": [
    "### 1.4. Search for  faulty testcases <a id=\"1-4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c973996",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs.orig_id.value_counts()[:30].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff42315",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_id = 69362\n",
    "\n",
    "diffs[diffs[\"orig_id\"] == orig_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d343043",
   "metadata": {},
   "source": [
    "## 2. Compare errors of two experiments <a id=\"2\"></a>\n",
    "\n",
    "Change `PATH1`, `PATH2` and `TEST_PATH1` for your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043b6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH1 = \"../results-archive/results_crawl_na_big_rework/crawl-na/xlm-roberta-base/\"\n",
    "PATH2 = \"../results-archive/results_exp_crawl_na_ft_da_10epochs/crawl-mlm/\"\n",
    "\n",
    "TEST_PATH1 = f\"crawl-na/crawl-na-test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0004b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test1, preds1 = get_test_and_preds(PATH1, TEST_PATH1)\n",
    "test2, preds2 = get_test_and_preds(PATH2, TEST_PATH1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7db8e5",
   "metadata": {},
   "source": [
    "### 2.1. Differences between True and Pred <a id=\"2-1\"></a>\n",
    "\n",
    "Normalizing = fixing white space errors. So `diffs` and `normalized_diffs` should ideally be the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2301048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test1 = join_test_preds(test1, preds1)\n",
    "new_test2 = join_test_preds(test2, preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f2b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs1, normalized_diffs1 = get_diffs_dfs(new_test1)\n",
    "diffs_no_sub1 = new_test1[(~new_test1.preds.isin(new_test1.true_answer))|(~new_test1.true_answer.isin(new_test1.preds))]\n",
    "\n",
    "print(\"(1) Count of wrong predictions:\", diffs1.shape[0])\n",
    "print(\"(1) Count of wrong predictions (with normalizing):\", normalized_diffs1.shape[0])\n",
    "print(\"(1) Count of wrong predictions (no substring match):\", diffs_no_sub1.shape[0])\n",
    "\n",
    "print()\n",
    "\n",
    "diffs2, normalized_diffs2 = get_diffs_dfs(new_test2)\n",
    "diffs_no_sub2 = new_test2[(~new_test2.preds.isin(new_test2.true_answer))|(~new_test2.true_answer.isin(new_test2.preds))]\n",
    "\n",
    "print(\"(2) Count of wrong predictions:\", diffs2.shape[0])\n",
    "print(\"(2) Count of wrong predictions (with normalizing):\", normalized_diffs2.shape[0])\n",
    "print(\"(2) Count of wrong predictions (no substring match):\", diffs_no_sub2.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b91ce64",
   "metadata": {},
   "source": [
    "### 2.2. Looking into ids of same and different errors <a id=\"2-2\"></a>\n",
    "\n",
    "Based on normalized diffs (substring match is possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a91fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_ids = []\n",
    "unique_ids_test1 = []\n",
    "unique_ids_test2 = []\n",
    "\n",
    "for _, row in normalized_diffs1.iterrows():\n",
    "    rid = row[\"id\"]\n",
    "    if rid in normalized_diffs2[\"id\"].values:\n",
    "        same_ids.append(rid)\n",
    "    else:\n",
    "        unique_ids_test1.append(rid)\n",
    "        \n",
    "for _, row in normalized_diffs2.iterrows():\n",
    "    rid = row[\"id\"]\n",
    "    if rid in normalized_diffs1[\"id\"].values:\n",
    "        if rid not in same_ids:\n",
    "            same_ids.append(rid)\n",
    "    else:\n",
    "        unique_ids_test2.append(rid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca802b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of the same errors on the basis of Test-1: {np.round(len(same_ids)/len(normalized_diffs1), decimals=3)}\")\n",
    "print(f\"Percentage of the same errors on the basis of Test-2: {np.round(len(same_ids)/len(normalized_diffs2), decimals=3)}\")\n",
    "#print(f\"Percentage of the different errors: {np.round(1 - (len(same_ids)/len(normalized_diffs1)), decimals=3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cbd809",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_errors_diffs1 = normalized_diffs1[normalized_diffs1[\"id\"].isin(unique_ids_test1)]\n",
    "unique_errors_diffs2 = normalized_diffs2[normalized_diffs2[\"id\"].isin(unique_ids_test2)]\n",
    "\n",
    "#print(\"\")\n",
    "print(f\"There are {len(unique_errors_diffs1)} unique errors for Test-1 when compared with Test-2.\")\n",
    "print(f\"There are {len(unique_errors_diffs2)} unique errors for Test-2 when compared with Test-1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9302a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter rune errors (should ideally be the same as unique_errors_diffs)\n",
    "unique_errors_diffs1_rune = unique_errors_diffs1[~unique_errors_diffs1.true_answer.str.contains(\"ᛉ\")]\n",
    "print(f\"There are overall {len(unique_errors_diffs1_rune)} different errors when comparing the two test dataframes (filtering runes).\")\n",
    "\n",
    "unique_errors_diffs2_rune = unique_errors_diffs2[~unique_errors_diffs2.true_answer.str.contains(\"ᛉ\")]\n",
    "print(f\"There are overall {len(unique_errors_diffs2_rune)} different errors when comparing the two test dataframes (filtering runes).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a0257",
   "metadata": {},
   "source": [
    "#### 2.2.1. Deeper analysis: Randomly looking into errors  <a id=\"2-2-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd18d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test1\n",
    "\n",
    "n = input()\n",
    "if n == \"\" or n == \"r\":\n",
    "    n = random.randint(0, len(unique_errors_diffs1))\n",
    "    print(\"n:\", n)\n",
    "current_error = unique_errors_diffs1.iloc[int(n)-1:int(n)]\n",
    "print(current_error.iloc[0][\"id\"])\n",
    "current_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db47070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test2\n",
    "\n",
    "n = input()\n",
    "if n == \"\" or n == \"r\":\n",
    "    n = random.randint(0, len(unique_errors_diffs1))\n",
    "    print(\"n:\", n)\n",
    "current_error2 = unique_errors_diffs2.iloc[int(n)-1:int(n)]\n",
    "print(current_error2.iloc[0][\"id\"])\n",
    "current_error2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42885d92",
   "metadata": {},
   "source": [
    "#### 2.2.2. Deeper analysis: N-Best Prediction analysis  <a id=\"2-2-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e416358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(PATH1+\"test_nbest_predictions.json\", \"r\") as f:\n",
    "    nbest_preds1 = json.load(f)\n",
    "    \n",
    "with open(PATH2+\"test_nbest_predictions.json\", \"r\") as f:\n",
    "    nbest_preds2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac21ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "given_id = \"69669-0\"\n",
    "\n",
    "print(\"Test-1\")\n",
    "print(\"------------------------\")\n",
    "for nb in nbest_preds1[given_id][:7]:\n",
    "    print(nb)\n",
    "\n",
    "print()\n",
    "print(\"Test-2\")\n",
    "print(\"------------------------\")\n",
    "for nb in nbest_preds2[given_id][:7]:\n",
    "    print(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca57e5c3",
   "metadata": {},
   "source": [
    "### 2.3. McNemar Test <a id=\"2-3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f89061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = calculate_p_value(normalized_diffs1, normalized_diffs2)\n",
    "print(\"p-value:\", p_value)\n",
    "p_value = np.round(p_value, decimals=4)\n",
    "alpha = 0.05\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(f\"Significant differences between errors. The p value {p_value} is smaller than Threshold α = {alpha}.\")\n",
    "else:\n",
    "    print(f\"No significant differences between errors. The p value {p_value} is higher than Threshold α = {alpha}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc272a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
